{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d174aef-4f00-4935-b84d-2db500039fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Memory growth enabled\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Initial Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Dropout, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enable memory growth for GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"Using GPU:\", physical_devices[0])\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"Memory growth enabled\")\n",
    "    except:\n",
    "        print(\"Invalid GPU device or cannot modify virtual devices once initialized\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n",
    "\n",
    "# Custom parameters\n",
    "IMAGE_SIZE = 128  # Resize images to 128x128\n",
    "BATCH_SIZE = 16   # Reduced batch size to avoid memory issues\n",
    "EPOCHS = 1      # Number of epochs\n",
    "WINDOW_LEVEL = 40  # Brain window level\n",
    "WINDOW_WIDTH = 120  # Brain window width\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Define paths (no sample limit)\n",
    "NORMAL_DIR = r\"C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\Normal\"  # Update this to your actual normal directory\n",
    "HEMORRHAGE_DIR = r\"C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\HEMORRHAGES CT AI\\Hemorrhage\"  # Update this to your actual hemorrhage directory\n",
    "OUTPUT_DIR = r\"C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\UNET\"  # Output directory for models only\n",
    "# Create output directory structure (only for models)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'models'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ee7f92-5aa2-408d-9507-dae7c4586167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def window_ct(image, window_level=40, window_width=120):\n",
    "    window_min = window_level - window_width / 2\n",
    "    window_max = window_level + window_width / 2\n",
    "    \n",
    "    windowed = (image - window_min) * (255 / (window_max - window_min))\n",
    "    windowed[windowed < 0] = 0\n",
    "    windowed[windowed > 255] = 255\n",
    "    \n",
    "    return windowed.astype(np.uint8)\n",
    "\n",
    "def load_dicom(path, target_size=(128, 128)):\n",
    "    try:\n",
    "        # Load DICOM file\n",
    "        dicom = pydicom.dcmread(path)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img = dicom.pixel_array.astype(np.float32)\n",
    "        \n",
    "        # Apply windowing\n",
    "        img = window_ct(img, WINDOW_LEVEL, WINDOW_WIDTH)\n",
    "        \n",
    "        # Resize\n",
    "        img = cv2.resize(img, target_size)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        img = img / 255.0\n",
    "        \n",
    "        # Add channel dimension if needed\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "            \n",
    "        return img\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95751a5a-b82a-4dc6-b553-ffd8d4b2d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting DICOM paths...\n",
      "Scanning directory: C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\Normal\n",
      "Found 34 patient folders, 45 date folders, 35539 DICOM files\n",
      "Scanning directory: C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\HEMORRHAGES CT AI\\Hemorrhage\n",
      "Found 52 patient folders, 74 date folders, 35952 DICOM files\n",
      "Found 35539 normal DICOM files.\n",
      "Found 35952 hemorrhage DICOM files.\n"
     ]
    }
   ],
   "source": [
    "# Get DICOM Paths with Reduced Debugging Output\n",
    "def get_dicom_paths(base_dir):\n",
    "    dicom_paths = []\n",
    "    print(f\"Scanning directory: {base_dir}\")\n",
    "    \n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"Error: Directory {base_dir} does not exist!\")\n",
    "        return dicom_paths\n",
    "    \n",
    "    patient_count = 0\n",
    "    date_count = 0\n",
    "    file_count = 0\n",
    "    \n",
    "    for patient_dir in os.listdir(base_dir):\n",
    "        patient_path = os.path.join(base_dir, patient_dir)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "        patient_count += 1\n",
    "        \n",
    "        for date_dir in os.listdir(patient_path):\n",
    "            date_path = os.path.join(patient_path, date_dir)\n",
    "            if not os.path.isdir(date_path):\n",
    "                continue\n",
    "            date_count += 1\n",
    "            \n",
    "            for file in os.listdir(date_path):\n",
    "                if file.lower().endswith(('.dcm', '.DCM', '.dic', '.DIC')):\n",
    "                    full_path = os.path.join(date_path, file)\n",
    "                    dicom_paths.append(full_path)\n",
    "                    file_count += 1\n",
    "    \n",
    "    print(f\"Found {patient_count} patient folders, {date_count} date folders, {file_count} DICOM files\")\n",
    "    return dicom_paths\n",
    "\n",
    "print(\"Getting DICOM paths...\")\n",
    "normal_paths = get_dicom_paths(NORMAL_DIR)\n",
    "hemorrhage_paths = get_dicom_paths(HEMORRHAGE_DIR)\n",
    "\n",
    "print(f\"Found {len(normal_paths)} normal DICOM files.\")\n",
    "print(f\"Found {len(hemorrhage_paths)} hemorrhage DICOM files.\")\n",
    "\n",
    "if len(normal_paths) == 0:\n",
    "    print(\"Warning: No normal DICOM files found. Check if NORMAL_DIR is correct and contains .dcm files.\")\n",
    "if len(hemorrhage_paths) == 0:\n",
    "    print(\"Warning: No hemorrhage DICOM files found. Check if HEMORRHAGE_DIR is correct and contains .dcm files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82781982-94ca-4596-abdd-170e22e5c281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting DICOM paths...\n",
      "Scanning directory: C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\Normal\n",
      "Found 34 patient folders, 45 date folders, 35539 DICOM files\n",
      "Scanning directory: C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\HEMORRHAGES CT AI\\Hemorrhage\n",
      "Found 52 patient folders, 74 date folders, 35952 DICOM files\n",
      "Found 35539 normal DICOM files.\n",
      "Found 35952 hemorrhage DICOM files.\n"
     ]
    }
   ],
   "source": [
    "# Get DICOM Paths with Reduced Debugging Output\n",
    "def get_dicom_paths(base_dir):\n",
    "    dicom_paths = []\n",
    "    print(f\"Scanning directory: {base_dir}\")\n",
    "    \n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"Error: Directory {base_dir} does not exist!\")\n",
    "        return dicom_paths\n",
    "    \n",
    "    patient_count = 0\n",
    "    date_count = 0\n",
    "    file_count = 0\n",
    "    \n",
    "    for patient_dir in os.listdir(base_dir):\n",
    "        patient_path = os.path.join(base_dir, patient_dir)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "        patient_count += 1\n",
    "        \n",
    "        for date_dir in os.listdir(patient_path):\n",
    "            date_path = os.path.join(patient_path, date_dir)\n",
    "            if not os.path.isdir(date_path):\n",
    "                continue\n",
    "            date_count += 1\n",
    "            \n",
    "            for file in os.listdir(date_path):\n",
    "                if file.lower().endswith(('.dcm', '.DCM', '.dic', '.DIC')):\n",
    "                    full_path = os.path.join(date_path, file)\n",
    "                    dicom_paths.append(full_path)\n",
    "                    file_count += 1\n",
    "    \n",
    "    print(f\"Found {patient_count} patient folders, {date_count} date folders, {file_count} DICOM files\")\n",
    "    return dicom_paths\n",
    "\n",
    "print(\"Getting DICOM paths...\")\n",
    "normal_paths = get_dicom_paths(NORMAL_DIR)\n",
    "hemorrhage_paths = get_dicom_paths(HEMORRHAGE_DIR)\n",
    "\n",
    "print(f\"Found {len(normal_paths)} normal DICOM files.\")\n",
    "print(f\"Found {len(hemorrhage_paths)} hemorrhage DICOM files.\")\n",
    "\n",
    "if len(normal_paths) == 0:\n",
    "    print(\"Warning: No normal DICOM files found. Check if NORMAL_DIR is correct and contains .dcm files.\")\n",
    "if len(hemorrhage_paths) == 0:\n",
    "    print(\"Warning: No hemorrhage DICOM files found. Check if HEMORRHAGE_DIR is correct and contains .dcm files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c8e52e-06a3-44a0-aee8-5ac50dd81753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 57192\n",
      "Validation samples: 14299\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data (Split and Load)\n",
    "if len(normal_paths) == 0 and len(hemorrhage_paths) == 0:\n",
    "    raise ValueError(\"No DICOM files found in either directory. Please check the paths and dataset.\")\n",
    "\n",
    "if len(normal_paths) == 0:\n",
    "    print(\"Warning: No normal DICOM files found. Proceeding with only hemorrhage data.\")\n",
    "    train_paths = hemorrhage_paths\n",
    "    val_paths = []\n",
    "    train_labels = [1] * len(hemorrhage_paths)  # All labels are hemorrhage (1)\n",
    "    val_labels = []\n",
    "    num_train_samples = len(train_paths)\n",
    "    num_val_samples = 0\n",
    "elif len(hemorrhage_paths) == 0:\n",
    "    print(\"Warning: No hemorrhage DICOM files found. Proceeding with only normal data.\")\n",
    "    train_paths = normal_paths\n",
    "    val_paths = []\n",
    "    train_labels = [0] * len(normal_paths)  # All labels are normal (0)\n",
    "    val_labels = []\n",
    "    num_train_samples = len(train_paths)\n",
    "    num_val_samples = 0\n",
    "else:\n",
    "    # Split data into train and validation sets\n",
    "    normal_train, normal_val = train_test_split(normal_paths, test_size=VALIDATION_SPLIT, random_state=42)\n",
    "    hemorrhage_train, hemorrhage_val = train_test_split(hemorrhage_paths, test_size=VALIDATION_SPLIT, random_state=42)\n",
    "\n",
    "    train_paths = normal_train + hemorrhage_train\n",
    "    val_paths = normal_val + hemorrhage_val\n",
    "    train_labels = [0] * len(normal_train) + [1] * len(hemorrhage_train)\n",
    "    val_labels = [0] * len(normal_val) + [1] * len(hemorrhage_val)\n",
    "\n",
    "    # Shuffle training data\n",
    "    train_indices = np.random.permutation(len(train_paths))\n",
    "    train_paths = [train_paths[i] for i in train_indices]\n",
    "    train_labels = [train_labels[i] for i in train_indices]\n",
    "\n",
    "    num_train_samples = len(train_paths)\n",
    "    num_val_samples = len(val_paths)\n",
    "\n",
    "print(f\"Training samples: {num_train_samples}\")\n",
    "print(f\"Validation samples: {num_val_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ccac3ca-385a-497b-9ecd-a6f03f60544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating U-Net model...\n"
     ]
    }
   ],
   "source": [
    "# Define U-Net Model\n",
    "def unet_model(input_size=(128, 128, 1), learning_rate=1e-5, decay_rate=1e-7):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    \n",
    "    # Middle\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    # Decoder\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(drop5)\n",
    "    )\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "    \n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv6)\n",
    "    )\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "    \n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv7)\n",
    "    )\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "    \n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv8)\n",
    "    )\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    \n",
    "    # Output\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Custom jaccard loss\n",
    "    def jaccard_loss(y_true, y_pred):\n",
    "        intersection = K.sum(y_true * y_pred)\n",
    "        union = K.sum(y_true) + K.sum(y_pred) - intersection\n",
    "        return 1 - intersection / (union + K.epsilon())\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-5, decay=1e-7),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', jaccard_loss]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Creating U-Net model...\")\n",
    "model = unet_model(input_size=(IMAGE_SIZE, IMAGE_SIZE, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4947bab-ae9c-4d29-a80b-7d712434ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-gdcm in c:\\users\\atharva badgujar\\appdata\\roaming\\python\\python39\\site-packages (3.0.24.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyjpegls in c:\\users\\atharva badgujar\\appdata\\roaming\\python\\python39\\site-packages (1.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pylibjpeg-libjpeg in c:\\users\\atharva badgujar\\appdata\\roaming\\python\\python39\\site-packages (2.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-gdcm --no-deps\n",
    "!pip install pyjpegls --no-deps\n",
    "!pip install pylibjpeg-libjpeg --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9edba6ca-5cf5-4da2-9fdf-560bbe405db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|██████████████████████████████████████████████████████████▍         | 3075/3575 [40:28<06:09,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\Normal\\JAYASHRI_JAGTAP___54Y_F 3913072\\2024-08-06 161531_001\\IMG-0005-00001.dcm: Unable to convert the pixel data: one of Pixel Data, Float Pixel Data or Double Float Pixel Data must be present in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 3575/3575 [47:30<00:00,  1.25it/s]\n",
      "Validation:  16%|██████████▌                                                         | 139/894 [01:27<07:39,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\Normal\\JAYASHRI_JAGTAP___54Y_F 3913072\\2024-08-06 161531\\IMG-0005-00001.dcm: Unable to convert the pixel data: one of Pixel Data, Float Pixel Data or Double Float Pixel Data must be present in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 894/894 [12:07<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2314 - accuracy: 0.8679 - jaccard_loss: 0.6682 - val_loss: 0.2261 - val_accuracy: 0.8910 - val_jaccard_loss: 0.7641\n",
      "Time: 3578.23s\n"
     ]
    }
   ],
   "source": [
    "# Train Model (Only Save Models)\n",
    "if 'num_train_samples' not in globals():\n",
    "    raise NameError(\"num_train_samples is not defined. Ensure Cell 4 runs successfully.\")\n",
    "\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'accuracy': [],\n",
    "    'jaccard_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': [],\n",
    "    'val_jaccard_loss': []\n",
    "}\n",
    "\n",
    "num_train_batches = (num_train_samples + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "num_val_batches = (num_val_samples + BATCH_SIZE - 1) // BATCH_SIZE if num_val_samples > 0 else 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_jaccard = 0\n",
    "    \n",
    "    for batch_idx in tqdm(range(num_train_batches), desc=\"Training\"):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = min(start_idx + BATCH_SIZE, num_train_samples)\n",
    "        \n",
    "        batch_paths = train_paths[start_idx:end_idx]\n",
    "        batch_labels = train_labels[start_idx:end_idx]\n",
    "        \n",
    "        batch_images = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, path in enumerate(batch_paths):\n",
    "            img = load_dicom(path, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            if img is not None:\n",
    "                batch_images.append(img)\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        batch_labels = [batch_labels[i] for i in valid_indices]\n",
    "        \n",
    "        if len(batch_images) == 0:\n",
    "            continue\n",
    "            \n",
    "        images = np.array(batch_images)\n",
    "        labels = np.array(batch_labels)\n",
    "        \n",
    "        masks = np.zeros_like(images)\n",
    "        for i, label in enumerate(labels):\n",
    "            if label == 1:\n",
    "                masks[i] = (images[i] > 0.7).astype(np.float32)\n",
    "        \n",
    "        batch_history = model.train_on_batch(images, masks)\n",
    "        \n",
    "        epoch_loss += batch_history[0]\n",
    "        epoch_acc += batch_history[1]\n",
    "        epoch_jaccard += batch_history[2]\n",
    "    \n",
    "    epoch_loss /= num_train_batches\n",
    "    epoch_acc /= num_train_batches\n",
    "    epoch_jaccard /= num_train_batches\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_jaccard = 0\n",
    "    \n",
    "    if num_val_samples > 0:\n",
    "        for batch_idx in tqdm(range(num_val_batches), desc=\"Validation\"):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = min(start_idx + BATCH_SIZE, num_val_samples)\n",
    "            \n",
    "            batch_paths = val_paths[start_idx:end_idx]\n",
    "            batch_labels = val_labels[start_idx:end_idx]\n",
    "            \n",
    "            batch_images = []\n",
    "            valid_indices = []\n",
    "            \n",
    "            for i, path in enumerate(batch_paths):\n",
    "                img = load_dicom(path, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "                if img is not None:\n",
    "                    batch_images.append(img)\n",
    "                    valid_indices.append(i)\n",
    "            \n",
    "            batch_labels = [batch_labels[i] for i in valid_indices]\n",
    "            \n",
    "            if len(batch_images) == 0:\n",
    "                continue\n",
    "                \n",
    "            images = np.array(batch_images)\n",
    "            labels = np.array(batch_labels)\n",
    "            \n",
    "            masks = np.zeros_like(images)\n",
    "            for i, label in enumerate(labels):\n",
    "                if label == 1:\n",
    "                    masks[i] = (images[i] > 0.7).astype(np.float32)\n",
    "            \n",
    "            batch_val_history = model.test_on_batch(images, masks)\n",
    "            \n",
    "            val_loss += batch_val_history[0]\n",
    "            val_acc += batch_val_history[1]\n",
    "            val_jaccard += batch_val_history[2]\n",
    "        \n",
    "        val_loss /= num_val_batches\n",
    "        val_acc /= num_val_batches\n",
    "        val_jaccard /= num_val_batches\n",
    "    \n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['accuracy'].append(epoch_acc)\n",
    "    history['jaccard_loss'].append(epoch_jaccard)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_acc)\n",
    "    history['val_jaccard_loss'].append(val_jaccard)\n",
    "    \n",
    "    print(f\"loss: {epoch_loss:.4f} - accuracy: {epoch_acc:.4f} - jaccard_loss: {epoch_jaccard:.4f} - \"\n",
    "          f\"val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f} - val_jaccard_loss: {val_jaccard:.4f}\")\n",
    "    print(f\"Time: {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    model.save(os.path.join(OUTPUT_DIR, 'models', f'unet_epoch_{epoch+1}.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f7d83a0-baa7-4859-a69a-5b7e57d050ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Save Final Model\n",
    "model.save(os.path.join(OUTPUT_DIR, 'models', 'unet_final.h5'))\n",
    "print(\"Final model saved.\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f301610-c3ab-4be7-ab1a-c1bc946984e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe115e-7407-4c5f-bebc-44785184a7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c7cb70-984a-4745-aa0d-4cb4f29cd751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading label 1: 100%|█████████████████████████████████████████████████████████████████| 52/52 [00:06<00:00,  8.39it/s]\n",
      "Loading label 0: 100%|█████████████████████████████████████████████████████████████████| 34/34 [00:04<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 159ms/step - loss: 0.6919 - accuracy: 0.5441 - val_loss: 0.6973 - val_accuracy: 0.3889\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6857 - accuracy: 0.6618 - val_loss: 0.7042 - val_accuracy: 0.3889\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6773 - accuracy: 0.6618 - val_loss: 0.7226 - val_accuracy: 0.3889\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6640 - accuracy: 0.6618 - val_loss: 0.7624 - val_accuracy: 0.3889\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6566 - accuracy: 0.6618 - val_loss: 0.7918 - val_accuracy: 0.3889\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6473 - accuracy: 0.6618 - val_loss: 0.7708 - val_accuracy: 0.3889\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6460 - accuracy: 0.6618 - val_loss: 0.7675 - val_accuracy: 0.3889\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6378 - accuracy: 0.6618 - val_loss: 0.7712 - val_accuracy: 0.3889\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6428 - accuracy: 0.6618 - val_loss: 0.7750 - val_accuracy: 0.3889\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.6465 - accuracy: 0.6618 - val_loss: 0.7746 - val_accuracy: 0.3889\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6421 - accuracy: 0.6618 - val_loss: 0.7813 - val_accuracy: 0.3889\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.6475 - accuracy: 0.6618 - val_loss: 0.8003 - val_accuracy: 0.3889\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6385 - accuracy: 0.6618 - val_loss: 0.8055 - val_accuracy: 0.3889\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6313 - accuracy: 0.6618 - val_loss: 0.8302 - val_accuracy: 0.3889\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6459 - accuracy: 0.6618 - val_loss: 0.8556 - val_accuracy: 0.3889\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6403 - accuracy: 0.6618 - val_loss: 0.8592 - val_accuracy: 0.3889\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.6524 - accuracy: 0.6618 - val_loss: 0.8232 - val_accuracy: 0.3889\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6441 - accuracy: 0.6618 - val_loss: 0.7988 - val_accuracy: 0.3889\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6452 - accuracy: 0.6618 - val_loss: 0.7795 - val_accuracy: 0.3889\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6408 - accuracy: 0.6618 - val_loss: 0.7659 - val_accuracy: 0.3889\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6493 - accuracy: 0.6618 - val_loss: 0.7572 - val_accuracy: 0.3889\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6460 - accuracy: 0.6618 - val_loss: 0.7539 - val_accuracy: 0.3889\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6459 - accuracy: 0.6618 - val_loss: 0.7552 - val_accuracy: 0.3889\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6529 - accuracy: 0.6618 - val_loss: 0.7615 - val_accuracy: 0.3889\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6563 - accuracy: 0.6618 - val_loss: 0.7636 - val_accuracy: 0.3889\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6483 - accuracy: 0.6618 - val_loss: 0.7647 - val_accuracy: 0.3889\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.6407 - accuracy: 0.6618 - val_loss: 0.7667 - val_accuracy: 0.3889\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6406 - accuracy: 0.6618 - val_loss: 0.7659 - val_accuracy: 0.3889\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.6469 - accuracy: 0.6618 - val_loss: 0.7550 - val_accuracy: 0.3889\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.6524 - accuracy: 0.6618 - val_loss: 0.7487 - val_accuracy: 0.3889\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.6508 - accuracy: 0.6618 - val_loss: 0.7434 - val_accuracy: 0.3889\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6485 - accuracy: 0.6618 - val_loss: 0.7419 - val_accuracy: 0.3889\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6489 - accuracy: 0.6618 - val_loss: 0.7457 - val_accuracy: 0.3889\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.6505 - accuracy: 0.6618 - val_loss: 0.7610 - val_accuracy: 0.3889\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6470 - accuracy: 0.6618 - val_loss: 0.7842 - val_accuracy: 0.3889\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6481 - accuracy: 0.6618 - val_loss: 0.8160 - val_accuracy: 0.3889\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6414 - accuracy: 0.6618 - val_loss: 0.8637 - val_accuracy: 0.3889\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6467 - accuracy: 0.6618 - val_loss: 0.9035 - val_accuracy: 0.3889\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.6609 - accuracy: 0.6618 - val_loss: 0.8803 - val_accuracy: 0.3889\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6544 - accuracy: 0.6618 - val_loss: 0.8252 - val_accuracy: 0.3889\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6397 - accuracy: 0.6618 - val_loss: 0.7858 - val_accuracy: 0.3889\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6410 - accuracy: 0.6618 - val_loss: 0.7637 - val_accuracy: 0.3889\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6424 - accuracy: 0.6618 - val_loss: 0.7585 - val_accuracy: 0.3889\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6472 - accuracy: 0.6618 - val_loss: 0.7603 - val_accuracy: 0.3889\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6500 - accuracy: 0.6618 - val_loss: 0.7651 - val_accuracy: 0.3889\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6351 - accuracy: 0.6618 - val_loss: 0.7790 - val_accuracy: 0.3889\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6376 - accuracy: 0.6618 - val_loss: 0.7967 - val_accuracy: 0.3889\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.6388 - accuracy: 0.6618 - val_loss: 0.8029 - val_accuracy: 0.3889\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6552 - accuracy: 0.6618 - val_loss: 0.7987 - val_accuracy: 0.3889\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.6451 - accuracy: 0.6618 - val_loss: 0.7893 - val_accuracy: 0.3889\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6480 - accuracy: 0.6618 - val_loss: 0.7861 - val_accuracy: 0.3889\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6412 - accuracy: 0.6618 - val_loss: 0.7795 - val_accuracy: 0.3889\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6466 - accuracy: 0.6618 - val_loss: 0.7757 - val_accuracy: 0.3889\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6410 - accuracy: 0.6618 - val_loss: 0.7798 - val_accuracy: 0.3889\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6361 - accuracy: 0.6618 - val_loss: 0.7771 - val_accuracy: 0.3889\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6363 - accuracy: 0.6618 - val_loss: 0.7774 - val_accuracy: 0.3889\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6428 - accuracy: 0.6618 - val_loss: 0.7786 - val_accuracy: 0.3889\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6427 - accuracy: 0.6618 - val_loss: 0.7837 - val_accuracy: 0.3889\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6412 - accuracy: 0.6618 - val_loss: 0.7967 - val_accuracy: 0.3889\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6450 - accuracy: 0.6618 - val_loss: 0.8236 - val_accuracy: 0.3889\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6384 - accuracy: 0.6618 - val_loss: 0.8374 - val_accuracy: 0.3889\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6211 - accuracy: 0.6618 - val_loss: 0.8328 - val_accuracy: 0.3889\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6454 - accuracy: 0.6618 - val_loss: 0.8257 - val_accuracy: 0.3889\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.6389 - accuracy: 0.6618 - val_loss: 0.8137 - val_accuracy: 0.3889\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6327 - accuracy: 0.6618 - val_loss: 0.8066 - val_accuracy: 0.3889\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6336 - accuracy: 0.6618 - val_loss: 0.7894 - val_accuracy: 0.3889\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6357 - accuracy: 0.6618 - val_loss: 0.7700 - val_accuracy: 0.3889\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.6401 - accuracy: 0.6618 - val_loss: 0.7610 - val_accuracy: 0.3889\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.6408 - accuracy: 0.6618 - val_loss: 0.7543 - val_accuracy: 0.3889\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6453 - accuracy: 0.6618 - val_loss: 0.7483 - val_accuracy: 0.3889\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.6446 - accuracy: 0.6618 - val_loss: 0.7472 - val_accuracy: 0.3889\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6485 - accuracy: 0.6618 - val_loss: 0.7453 - val_accuracy: 0.3889\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.6443 - accuracy: 0.6618 - val_loss: 0.7476 - val_accuracy: 0.3889\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.6465 - accuracy: 0.6618 - val_loss: 0.7556 - val_accuracy: 0.3889\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6424 - accuracy: 0.6618 - val_loss: 0.7697 - val_accuracy: 0.3889\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6363 - accuracy: 0.6618 - val_loss: 0.7980 - val_accuracy: 0.3889\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6212 - accuracy: 0.6618 - val_loss: 0.8374 - val_accuracy: 0.3889\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.6472 - accuracy: 0.6618 - val_loss: 0.8460 - val_accuracy: 0.3889\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6568 - accuracy: 0.6618 - val_loss: 0.8240 - val_accuracy: 0.3889\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6214 - accuracy: 0.6618 - val_loss: 0.8087 - val_accuracy: 0.3889\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6308 - accuracy: 0.6618 - val_loss: 0.7950 - val_accuracy: 0.3889\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6406 - accuracy: 0.6618 - val_loss: 0.7910 - val_accuracy: 0.3889\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6373 - accuracy: 0.6618 - val_loss: 0.7950 - val_accuracy: 0.3889\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.6266 - accuracy: 0.6618 - val_loss: 0.8004 - val_accuracy: 0.3889\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6353 - accuracy: 0.6618 - val_loss: 0.7954 - val_accuracy: 0.3889\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.6318 - accuracy: 0.6618 - val_loss: 0.8028 - val_accuracy: 0.3889\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6176 - accuracy: 0.6618 - val_loss: 0.8304 - val_accuracy: 0.3889\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6383 - accuracy: 0.6618 - val_loss: 0.8606 - val_accuracy: 0.3889\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6484 - accuracy: 0.6618 - val_loss: 0.8527 - val_accuracy: 0.3889\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6325 - accuracy: 0.6618 - val_loss: 0.8101 - val_accuracy: 0.3889\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6240 - accuracy: 0.6618 - val_loss: 0.8001 - val_accuracy: 0.3889\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.6345 - accuracy: 0.6618 - val_loss: 0.7895 - val_accuracy: 0.3889\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6318 - accuracy: 0.6618 - val_loss: 0.7813 - val_accuracy: 0.3889\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6332 - accuracy: 0.6618 - val_loss: 0.7814 - val_accuracy: 0.3889\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6341 - accuracy: 0.6618 - val_loss: 0.7831 - val_accuracy: 0.3889\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6221 - accuracy: 0.6618 - val_loss: 0.7804 - val_accuracy: 0.3889\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6221 - accuracy: 0.6618 - val_loss: 0.7844 - val_accuracy: 0.3889\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6164 - accuracy: 0.6618 - val_loss: 0.7892 - val_accuracy: 0.3889\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6381 - accuracy: 0.6618 - val_loss: 0.8027 - val_accuracy: 0.3889\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6245 - accuracy: 0.6618 - val_loss: 0.8108 - val_accuracy: 0.3889\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        11\n",
      "           1       0.39      1.00      0.56         7\n",
      "\n",
      "    accuracy                           0.39        18\n",
      "   macro avg       0.19      0.50      0.28        18\n",
      "weighted avg       0.15      0.39      0.22        18\n",
      "\n",
      "Classifier model saved as classifier_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atharva Badgujar\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Atharva Badgujar\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Atharva Badgujar\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# === CONFIG ===\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "SEG_MODEL_PATH = 'models/unet_epoch_1.h5'\n",
    "HEMORRHAGE_PATH = r\"C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\HEMORRHAGES CT AI\\Hemorrhage\"\n",
    "NORMAL_PATH = r\"C:\\Users\\Atharva Badgujar\\OneDrive\\Desktop\\Hemorrhage new\\LSTM model\\Normal\"\n",
    "\n",
    "# === LOAD TRAINED UNET ===\n",
    "seg_model = load_model(SEG_MODEL_PATH, compile=False)\n",
    "\n",
    "# === LOAD DICOM IMAGE ===\n",
    "def load_dicom_image(path):\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    img = dcm.pixel_array.astype(np.float32)\n",
    "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-5)\n",
    "    return img\n",
    "\n",
    "# === APPLY SEGMENTATION MASK ===\n",
    "def generate_masked_input(image):\n",
    "    input_img = np.expand_dims(image, axis=(0, -1))  # (1, 128, 128, 1)\n",
    "    predicted_mask = seg_model.predict(input_img, verbose=0)[0, :, :, 0]\n",
    "    masked_image = image * predicted_mask\n",
    "    return masked_image\n",
    "\n",
    "# === FIND FIRST DICOM FILE IN FOLDER ===\n",
    "def get_first_dicom_path(folder):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "# === LOAD DATASET FROM SEPARATE FOLDERS ===\n",
    "def load_dataset_from_path(class_path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for patient in tqdm(os.listdir(class_path), desc=f\"Loading label {label}\"):\n",
    "        patient_folder = os.path.join(class_path, patient)\n",
    "        dcm_path = get_first_dicom_path(patient_folder)\n",
    "        if dcm_path:\n",
    "            try:\n",
    "                img = load_dicom_image(dcm_path)\n",
    "                masked_img = generate_masked_input(img)\n",
    "                images.append(masked_img)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {dcm_path}: {e}\")\n",
    "                continue\n",
    "    return images, labels\n",
    "\n",
    "# === LOAD ALL DATA ===\n",
    "X_h, y_h = load_dataset_from_path(HEMORRHAGE_PATH, 1)\n",
    "X_n, y_n = load_dataset_from_path(NORMAL_PATH, 0)\n",
    "\n",
    "X = np.array(X_h + X_n).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "y = np.array(y_h + y_n)\n",
    "\n",
    "# === SPLIT ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === CLASSIFIER MODEL ===\n",
    "def build_classifier(input_shape=(128, 128, 1)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "clf_model = build_classifier()\n",
    "clf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === TRAIN ===\n",
    "clf_model.fit(X_train, y_train, epochs=100, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n",
    "\n",
    "# === EVALUATE ===\n",
    "preds = clf_model.predict(X_test) > 0.5\n",
    "print(classification_report(y_test, preds.astype(int)))\n",
    "\n",
    "# === SAVE CLASSIFIER MODEL ===\n",
    "clf_model.save(\"classifier_model.h5\")\n",
    "print(\"Classifier model saved as classifier_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79098ea5-33cd-4e6c-947e-14146224be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === CONFIG ===\n",
    "IMAGE_SIZE = 128\n",
    "TEST_DATASET_PATH = 'path_to_test_folder'  # Folder containing 'hemorrhagic' and 'normal'\n",
    "SEG_MODEL_PATH = 'your_trained_unet.h5'\n",
    "CLF_MODEL_PATH = 'classifier_model.h5'\n",
    "\n",
    "# === LOAD MODELS ===\n",
    "seg_model = load_model(SEG_MODEL_PATH, compile=False)\n",
    "clf_model = load_model(CLF_MODEL_PATH, compile=False)\n",
    "\n",
    "# === IMAGE PROCESSING ===\n",
    "def load_dicom_image(dcm_path):\n",
    "    dcm = pydicom.dcmread(dcm_path)\n",
    "    img = dcm.pixel_array.astype(np.float32)\n",
    "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-5)\n",
    "    return img\n",
    "\n",
    "def generate_mask(img):\n",
    "    input_img = np.expand_dims(img, axis=(0, -1))\n",
    "    pred_mask = seg_model.predict(input_img, verbose=0)[0, :, :, 0]\n",
    "    return (pred_mask > 0.5).astype(np.float32)\n",
    "\n",
    "def apply_mask(img, mask):\n",
    "    return img * mask\n",
    "\n",
    "def predict_label(masked_img):\n",
    "    input_img = np.expand_dims(masked_img, axis=(0, -1))\n",
    "    pred = clf_model.predict(input_img, verbose=0)[0][0]\n",
    "    return \"Hemorrhage\" if pred >= 0.5 else \"Normal\", float(pred)\n",
    "\n",
    "def get_first_dicom(folder_path):\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "# === RUN PREDICTION ===\n",
    "def predict_on_dataset(base_dir):\n",
    "    results = []\n",
    "\n",
    "    for true_label in [\"hemorrhagic\", \"normal\"]:\n",
    "        label_dir = os.path.join(base_dir, true_label)\n",
    "        if not os.path.isdir(label_dir):\n",
    "            print(f\"Skipping missing folder: {label_dir}\")\n",
    "            continue\n",
    "\n",
    "        for patient_id in tqdm(os.listdir(label_dir), desc=f\"Processing {true_label}\"):\n",
    "            patient_folder = os.path.join(label_dir, patient_id)\n",
    "            if not os.path.isdir(patient_folder):\n",
    "                continue\n",
    "\n",
    "            dcm_path = get_first_dicom(patient_folder)\n",
    "            if not dcm_path:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                img = load_dicom_image(dcm_path)\n",
    "                mask = generate_mask(img)\n",
    "                masked_img = apply_mask(img, mask)\n",
    "                pred_label, confidence = predict_label(masked_img)\n",
    "\n",
    "                results.append({\n",
    "                    \"PatientID\": patient_id,\n",
    "                    \"TrueLabel\": true_label,\n",
    "                    \"PredictedLabel\": pred_label,\n",
    "                    \"Confidence\": confidence\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {patient_id}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"segmentation_classification_results.csv\", index=False)\n",
    "    print(\"✅ Saved results to segmentation_classification_results.csv\")\n",
    "\n",
    "# === RUN ===\n",
    "predict_on_dataset(TEST_DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c887a0e-af47-4a04-9d44-9ef22b3133df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_prediction(img, mask, masked_img, pred, label):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "    axs[0].imshow(img, cmap='gray'); axs[0].set_title('Original')\n",
    "    axs[1].imshow(mask, cmap='gray'); axs[1].set_title('Predicted Mask')\n",
    "    axs[2].imshow(masked_img, cmap='gray'); axs[2].set_title(f'Masked - Pred: {label} ({pred:.2f})')\n",
    "    for ax in axs: ax.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b2f87c-cdea-4b23-856b-5225fa1ff27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# === Get True & Predicted Labels ===\n",
    "y_true = y_test\n",
    "y_pred = (clf_model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# === Plot ===\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Hemorrhage\"], yticklabels=[\"Normal\", \"Hemorrhage\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a54721c8-4d97-4954-836b-d7054d24c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "\n",
    "def build_efficientnet_classifier(input_shape=(128, 128, 3)):\n",
    "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    base_model.trainable = False  # freeze base model for transfer learning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)  # binary classification\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0bf944b-06b0-4938-9bdb-ab33728a3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_image(dcm_path):\n",
    "    dcm = pydicom.dcmread(dcm_path)\n",
    "    img = dcm.pixel_array.astype(np.float32)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-5)\n",
    "\n",
    "    # Convert grayscale to RGB by stacking\n",
    "    img = np.stack([img] * 3, axis=-1)  # shape becomes (128,128,3)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab16d7-c5c2-4996-9586-2d11faf25182",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = build_efficientnet_classifier()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=8)\n",
    "model.save(\"classifier_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a861959-4169-4fa6-b107-f41ed7d74084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c3d44-d0af-46a8-9faa-e0a858aeb83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
